{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Памятка для себя:\n",
        "- обучить модель - найти функцию, которая бы хорошо приближала исходную функцию\n",
        "- ищем в семействе линейных $$a(x) = \\langle w, x \\rangle + w_0$$\n",
        "- \"хорошо приближала\" - в смысле квадратичной функции потерь $$L(y, a(x)) = (a(x) - y)^2 \\to min$$\n",
        "- будем искать такие $w$ и $w_0$, которые бы минимизировали это выражение\n",
        "- будем искать градиентным спуском\n"
      ],
      "metadata": {
        "id": "3743XSLjQMC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сгенерируем синтетические данные для регрессии"
      ],
      "metadata": {
        "id": "wp-lVmR0Pdm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "Mp_EjyAg0rSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset for linear regression with random parameters generation\n",
        "\n",
        "features = np.random.randint(5, 51)\n",
        "informative = int(features * np.random.random_sample())\n",
        "noise = np.random.random_sample()\n",
        "bias = np.random.randint(0, 50)\n",
        "\n",
        "X, y = make_regression(n_samples=100, n_features=features, n_informative=informative, bias=bias, noise=noise, random_state=42)\n",
        "\n",
        "print(f'features = {features}, informative = {informative}, noise = {noise}, bias = {bias}')\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPGotLfhF617",
        "outputId": "a6b5c09e-7039-4b4c-9137-8cdacaf8479f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features = 23, informative = 20, noise = 0.948870958500644, bias = 25\n",
            "(100, 23) (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveGradientDescent:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1500, threshold=1e-6):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.threshold = threshold\n",
        "        self.losses = []\n",
        "\n",
        "    def calc_loss(self, X_train, y_train):\n",
        "        y_pred = np.dot(X_train, self.weights)\n",
        "        loss = np.sum((y_train - y_pred)**2) / len(y_train)\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        n = len(X_train)\n",
        "        X_train = np.concatenate([np.ones((n, 1)), X_train], axis=1)\n",
        "        self.weights = np.random.normal(0, 0.1, size=X_train.shape[1])\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            self.losses.append(self.calc_loss(X_train, y_train))\n",
        "\n",
        "            y_pred = np.dot(X_train, self.weights)\n",
        "            grad = (2 / n) * np.dot(X_train.T, (y_pred - y_train))\n",
        "\n",
        "            self.weights -= self.learning_rate * grad\n",
        "\n",
        "            if len(self.losses) >= 2:\n",
        "                if abs(self.losses[-1] - self.losses[-2]) < self.threshold:\n",
        "                    break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        n = len(X)\n",
        "        X = np.concatenate([np.ones((n, 1)), X], axis=1)\n",
        "        y_pred = np.dot(X, self.weights)\n",
        "        return y_pred\n",
        "\n",
        "    def weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def losses(self):\n",
        "        return self.losses"
      ],
      "metadata": {
        "id": "Lp5yM4XFFW2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_gd = NaiveGradientDescent()\n",
        "naive_gd.fit(X, y)\n",
        "y_pred_gd = naive_gd.predict(X)"
      ],
      "metadata": {
        "id": "_uX9sTe0FW4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'mse = {mean_squared_error(y, y_pred_gd)}, mae = {mean_absolute_error(y, y_pred_gd)}', end='\\n\\n')\n",
        "print(f'naive_grad.intercept_ = {naive_gd.weights[0]}', end='\\n\\n')\n",
        "print(naive_gd.weights[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KrIdAcYFW6u",
        "outputId": "bc63c27f-f2f8-4c6a-ad2c-2581e3af3633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mse = 0.6076780602357938, mae = 0.6399446517442745\n",
            "\n",
            "naive_grad.intercept_ = 24.931864051505993\n",
            "\n",
            "[ 2.94944984e+01  3.70050488e+01  9.19365546e+01  7.59817511e+01\n",
            " -1.11098524e-01  8.60913762e+01  3.45051447e+01  2.74278465e+01\n",
            "  8.26341935e+01  6.96972803e+01  1.94243349e+01 -8.50777762e-02\n",
            "  6.94510167e+01  7.13947117e+01  4.62033712e+01  7.28456554e+01\n",
            "  5.14171887e+01 -5.44063063e-02  8.08199802e+01  1.02569225e+01\n",
            "  9.34393880e+01  5.08380841e+01  7.14697583e+01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Для сравнения**"
      ],
      "metadata": {
        "id": "Sg_Zeai-dwSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linreg = LinearRegression()\n",
        "linreg.fit(X, y)\n",
        "y_pred = linreg.predict(X)"
      ],
      "metadata": {
        "id": "YuvjZcMFKUjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'mse = {mean_squared_error(y, y_pred)}, mae = {mean_absolute_error(y, y_pred)}', end='\\n\\n')\n",
        "print(f'linreg.intercept_ = {linreg.intercept_}', end='\\n\\n')\n",
        "print(linreg.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6FtBw5mKfao",
        "outputId": "b98268de-80af-4d75-b108-0059c85d0fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mse = 0.6076043810795444, mae = 0.6407862267751581\n",
            "\n",
            "linreg.intercept_ = 24.92542864656881\n",
            "\n",
            "[ 2.94962111e+01  3.70096838e+01  9.19413583e+01  7.59814180e+01\n",
            " -1.10129126e-01  8.60888544e+01  3.45057742e+01  2.74263445e+01\n",
            "  8.26412300e+01  6.96964957e+01  1.94223302e+01 -8.53262323e-02\n",
            "  6.94565976e+01  7.13939872e+01  4.62029179e+01  7.28484714e+01\n",
            "  5.14205778e+01 -5.62830965e-02  8.08212852e+01  1.02572049e+01\n",
            "  9.34406120e+01  5.08346842e+01  7.14712893e+01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jOJD_W0BmvCN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}